import re
import math  # Import the math module
import os

def evaluate_prism_code(file_path):
    """
    Evaluates PRISM code from a file based on various metrics, including an
    assessment of modeling effort.

    Args:
        file_path (str): The path to the PRISM code file.

    Returns:
        dict: A dictionary containing the evaluation metrics, including
              an estimated modeling effort and its potential range.
    """

    try:
        with open(file_path, 'r') as f:
            prism_code = f.read()
    except FileNotFoundError:
        return {"error": f"File not found: {file_path}"}
    except Exception as e:
        return {"error": f"An error occurred while reading the file: {e}"}

    metrics = {}

    # 1. Number of Modules
    modules = re.findall(r"module\s+(\w+)", prism_code)
    metrics["Number of Modules"] = len(modules)

    # 2. Number of Parameters
    parameters = re.findall(r"const\s+double\s+(\w+)\s*=", prism_code)
    metrics["Number of Parameters"] = len(parameters)

    # 3. Code Quality (Basic Heuristics)
    quality_metrics = {
        "Lines of Code": len(prism_code.splitlines()),
        "Comments": len(re.findall(r"//.*", prism_code)),
        "Long Lines (>80 chars)": len([line for line in prism_code.splitlines() if len(line) > 80]),
        "Empty Lines": len([line for line in prism_code.splitlines() if not line.strip()]),
    }
    metrics["Code Quality"] = quality_metrics

    # 4. Modeling Effort Assessment
    effort, min_effort, max_effort = assess_modeling_effort(metrics)
    metrics["Modeling Effort"] = effort
    metrics["Modeling Effort Range"] = {"min": min_effort, "max": max_effort}

    # Overall Quality Score (Example - You'll need to define your own logic)
    metrics["Overall Quality Score"] = calculate_overall_quality(metrics)

    return metrics


def assess_modeling_effort(metrics):
    """
    Estimates the modeling effort required based on various code characteristics.
    This is a heuristic-based approach and should be refined based on domain
    expertise and specific project context.  It also calculates a *rough*
    estimate of the possible range of the effort.

    Args:
        metrics (dict): The metrics dictionary generated by evaluate_prism_code().

    Returns:
        tuple: (estimated_effort, minimum_possible_effort, maximum_possible_effort)
    """

    effort = 0.0
    min_effort = 0.0
    max_effort = 0.0

    # Base effort based on modules and parameters
    module_effort = metrics["Number of Modules"] * 10
    effort += module_effort
    min_effort += module_effort  # Base is always included
    max_effort += module_effort  # Base is always included

    param_effort = metrics["Number of Parameters"] * 5
    effort += param_effort
    min_effort += param_effort
    max_effort += param_effort

    # Adjust effort based on code quality
    quality_metrics = metrics["Code Quality"]

    loc_effort = math.log2(quality_metrics["Lines of Code"] + 1) * 2  # Logarithmic penalty
    effort += loc_effort
    min_effort += 0  # Lines of code contribute positively
    max_effort += quality_metrics["Lines of Code"] * 0.2 # A linear penalty

    long_lines_effort = quality_metrics["Long Lines (>80 chars)"] * 3
    effort += long_lines_effort
    min_effort += 0  # Long lines contribute positively
    max_effort += long_lines_effort * 2 # Increase max penalty

    comment_reward = min(quality_metrics["Comments"], 10) * 1  # Cap the reward
    effort -= comment_reward
    min_effort -= comment_reward # Comments reduce effort
    max_effort -= 0 # Comments reduce effort

    # Normalize or scale the effort (example - adjust as needed)
    effort = effort / 50.0  # Scale to a reasonable range
    min_effort = min_effort / 50.0
    max_effort = max_effort / 50.0

    return effort, min_effort, max_effort


def calculate_overall_quality(metrics):
    """
    Calculates a simple overall quality score based on the metrics.
    This is a placeholder - you'll need to define your own scoring logic.

    Args:
        metrics (dict): The metrics dictionary.

    Returns:
        float: The overall quality score.
    """

    quality_metrics = metrics["Code Quality"]
    score = 100  # Start with a perfect score

    # Penalize for long lines and excessive empty lines
    score -= quality_metrics["Long Lines (>80 chars)"] * 2
    score -= quality_metrics["Empty Lines"] * 0.5

    # Reward for comments (up to a limit)
    score += min(quality_metrics["Comments"], 10) * 3

    # Penalize for lines of code (very basic - consider complexity instead)
    score -= quality_metrics["Lines of Code"] * 0.1  # Adjust penalty as needed

    # Normalize score to be within 0-100 range (crude example)
    score = max(0, min(100, score))

    return score


def print_structured_results(results, indent=""):
    """
    Prints the evaluation results in a structured, tree-like format.

    Args:
        results (dict): The dictionary of evaluation results.
        indent (str): The indentation string for the current level.
    """

    for key, value in results.items():
        if isinstance(value, dict):
            print(f"{indent}{key}:")
            print_structured_results(value, indent + "  ")  # Increase indent
        else:
            print(f"{indent}{key}: {value}")


# Example Usage
# Example Usage
file_path = "modelparameters.nm"  # Replace with the actual path to your PRISM file

evaluation_results = evaluate_prism_code(file_path)

if "error" in evaluation_results:
    print(f"Error: {evaluation_results['error']}")
else:
    print("Evaluation Results:")
    print_structured_results(evaluation_results)
